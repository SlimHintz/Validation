{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "![](https://www.mihaileric.com/static/model-selection-meme-bd4a6a86f615583d1a1bbc497ca4640e-67414.jpeg)\n",
    "\n",
    "This notebook is inspired from:\n",
    "[Hands on ML with Scikit-Learn and TensorFlow](https://github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Objectives\n",
    "\n",
    "\n",
    "By the end of this lesson students will be able to: \n",
    "\n",
    "- Perform Linear Regression with Sklearn\n",
    "\n",
    "- Transform target variables in linear regression\n",
    "\n",
    "- Conduct Model Selection\n",
    "\n",
    "- Conduct Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in our Data\n",
    "\n",
    "We are going to be using a dataset today that contains information about houses!  We are going to create a linear regression model to predict the prices of houses!\n",
    "\n",
    "![](https://media3.giphy.com/media/3oeHLmEqXbVrQtaGBy/giphy.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    \n",
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining our data\n",
    "\n",
    "Now that we have read in our data files let's take a quick look at what we have in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn and Talk\n",
    "\n",
    "In your group discuss the above histograms.  What do you notice?  What conclusions do you make?  Does anything seem odd?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data!  (aka Train-Test Split)\n",
    "\n",
    "Before we look at anything else related to our data we will split our data apart.  We want to avoid data-leakage by making this split early on so that we don't learn anything about the data that will lead you to make decisions about our model.  We will create this \"test set\" which will simulate new and unseen data.  Once we create this (at the very beginning of our analysis) we will \"forget about it\" until the end of the analysis.\n",
    "\n",
    "![](https://media2.giphy.com/media/l4FGyUShS5LTbbOmI/source.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we split data as train/test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The generalization performance__ of a learning method relates to its prediction capability on independent test data. Assessment of this performance is extremely important in practice, since it guides the choice of learning method or model, and gives us __a measure of the quality of the ultimately chosen model.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src  = img/train-test-validation.png width = 350/>\n",
    "\n",
    "<img src=img/test_train_split.png width =450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Homemade Train-Test Split Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# For illustration only. Sklearn has train_test_split()\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "#now let's split our data maintaining 20% in our test set\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(len(train_set), \"train +\", len(test_set), \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split with Scikit-Learn\n",
    "\n",
    "Luckily we won't need to create our own function because sklearn has built in the [`train_test_split` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## as you can see it is a function\n",
    "type(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note that this function produces the same result as our home-brewed function- although the indices are different\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "print(len(train_set), \"train +\", len(test_set), \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Splitting\n",
    "\n",
    "In some cases you may want to do stratified sampling instead of random sampling when making your splits.  In this sampling method the population is divided into homogeneous groups called strata and a proportionate number of instances are samples from each of the stratum to guarantee the test set is representative of the overall population.\n",
    "\n",
    "![](https://faculty.elgin.edu/dkernler/statistics/ch01/images/strata-sample.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our housing data we might have some expert knowledge that the median income is a very important feature in predicting median housing prices.  In this case we might want to ensure that our test set is representative of the various median income categories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's bin the median_income column into 5 different bins.\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "housing[\"income_cat\"].hist()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our income categories we can do a stratified test-train split.  This will ensure that we have sampled a representative number of data for each income category when we make our split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[housing.index.intersection(train_index)]\n",
    "    strat_test_set = housing.loc[housing.index.intersection(test_index)]\n",
    "\n",
    "strat_test_set.income_cat.value_counts()/len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: that when we use vanilla train-test split income distributions in the test set is not exactly the aligns with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "test_set.income_cat.value_counts()/len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also stratify our split by using the `stratify` argument in train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42, stratify=housing.income_cat)\n",
    "test_set.income_cat.value_counts()/len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare the different splitting techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table we see that the stratified technique will generate a test set that is nearly identical to the overall proportions, where the randomly generated test set is skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some data cleaning and feature engineering to prepare the data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing =train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "y = train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's take a look at rows with missing values:\n",
    "sample_incomplete_rows = train_set[train_set.isnull().any(axis=1)]\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We don't have any missing values!\n",
    "\n",
    "Now let's separate out the numeric columns from the categorical column(`ocean_proximity`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.drop('ocean_proximity', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's preprocess the categorical input features by dummy coding them!  We will use the `OneHotEncoder` option in sklearn to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[['ocean_proximity']]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "except ImportError:\n",
    "    from future_encoders import OneHotEncoder # Scikit-Learn < 0.20\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments we might want to adjust in `OneHotEncoder`\n",
    "\n",
    "`drop=first` :  this will drop the first category when creating the dummy coding and create a referent group\n",
    "\n",
    "`sparse == False` : this will return an array instead of space matrix\n",
    "\n",
    "And if we want to connect the categories back to our array we can access the `categories_` attribute which will return a list of the feature names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now join the categorical and numeric features back together in a dataframe with labels so we know which column is which!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[(housing_num, housing_cat_1hot.toarray())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = housing_num.columns.tolist() +cat_encoder.categories_[0].tolist()\n",
    "housing_tr = pd.DataFrame(X, columns=cols)\n",
    "housing_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = housing_tr\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Regression model\n",
    "\n",
    "Now that we have prepared our data we can start our modeling process by running a baseline model. A baseline model is a model with NO predictors.  In regression essentially it is the mean ( or median) of the y-variable.  We can create a baseline model in sklearn using the [`DummyRegressor` object](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy = DummyRegressor()  # by default this will use the mean\n",
    "\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "dummy.score(X_train, y_train) # the score of a regression model is the r-squared value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dummy.predict(X_train) #making predicted values of y based on our x values and our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the RMSE of our dummy model as another way to assess the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "dummy_rmse = mean_squared_error(y_train, y_pred, squared=False)\n",
    "dummy_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RSME here can be interpreted as the amount on average a data point differs from the line of best fit. This value is in units of the y variable so in our case the price of the house will differ from the predicted value on average of 115,619 dollars. That is a lot!  Let's see if we can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression model\n",
    "\n",
    "Now that we know our baseline results we can move on to modeling with our set of features.  The goal here is that we want to create a model that is better than our baseline model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import necessary tools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## prepare(Instantiate) LinearRegression to use\n",
    "lr = LinearRegression()\n",
    "\n",
    "## coefficients are learnt and stored in \"lr\" at this step\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the ols function in statsmodels the sklearn implementation does not have a summary table.  The `lr` object has all the information we need for this linear regression problem but we gave to dig a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check coefficients\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the intercept of the model\n",
    "\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rsme = mean_squared_error(y_train, y_pred, squared=False)\n",
    "lr_rsme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn and Talk!\n",
    "\n",
    "How did our linear regression model with our features do in relation to the dummy model?  Was it better?  Worse?  How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test Set!\n",
    "\n",
    "Now let's pretend this linear regression model is our \"final\" model (aka. we have tweaked the model and we have determined this one is the best according to our metrics).  Now let's use that model to make predictions using our test data!\n",
    "\n",
    "The first thing we need to do is to transform our training data in the same way as our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_test= test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test=test_set['median_house_value'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_test_num = housing_test.drop('ocean_proximity', axis =1)\n",
    "housing_test_cat = housing_test[['ocean_proximity']]\n",
    "housing_test_cat_1hot = cat_encoder.transform(housing_test_cat)  ####NOTE: we only transform our test data!\n",
    "housing_test_cat_1hot.toarray()\n",
    "X_test = np.c_[(housing_test_num, housing_test_cat_1hot.toarray())]\n",
    "X_test =pd.DataFrame(X_test, columns =cols)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to model!  __Note:  We don't fit our model on our training data!  We just use it to predict!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the ols function in statsmodels the sklearn implementation does not have a summary table.  The `lr` object has all the information we need for this linear regression problem but we gave to dig a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rsme = mean_squared_error(y_test, y_pred, squared=False)\n",
    "lr_rsme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that there are in fact two separate goals that we might have in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'img/model_selection.png' width = 550/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we have plenty of data then, the best approach is to randomly divide the dataset into three parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'img/validation_ratio.png' width = 550/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/cross-validation.png)\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "[Cross-validation-sklearn](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lr = LinearRegression()\n",
    "cross_val_score(estimator=lr, X=X, y=y, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output displays the score (here r-squared as the default) for each of the 5 folds.  We want to see this number stay the same and not fluctuate.  If it fluctuates than it's possible we have overfit our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other methods in sklearn with slightly different features for cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!!\n",
    "\n",
    "In your group, using the cleaned_movie_data.csv run a multiple linear regression model to predict gross revenue. Start with 3 continuous variables and one categorical predictor.\n",
    "\n",
    "- Be sure to do a test train split of your data\n",
    "- Use the training data to build and adjust your model- be sure to start with a baseline model and iterate from there\n",
    "- Make sure to look at evaluation metrics as you build your model\n",
    "- Once you have a \"final\" model, make new predictions with your test data and evaluate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Why train-validation-test splitting?](https://medium.com/datadriveninvestor/data-science-essentials-why-train-validation-test-data-b7f7d472dc1f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
